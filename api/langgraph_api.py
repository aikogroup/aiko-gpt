"""
API LangGraph pour le workflow d'analyse des besoins.
Architecture propre : Streamlit = UI, API = Logique m√©tier.
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
from typing import List, Optional, Dict, Any
import uvicorn
import uuid
import os
from pathlib import Path
import tempfile
from pydantic import BaseModel
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Importer les workflows
import sys
sys.path.append(str(Path(__file__).parent.parent))
from workflow.need_analysis_workflow import NeedAnalysisWorkflow
from workflow.rappel_mission_workflow import RappelMissionWorkflow
from workflow.atouts_workflow import AtoutsWorkflow
from workflow.value_chain_workflow import ValueChainWorkflow
from executive_summary.executive_summary_workflow import ExecutiveSummaryWorkflow
from prerequis_evaluation.prerequis_evaluation_workflow import PrerequisEvaluationWorkflow
from langgraph.checkpoint.memory import MemorySaver
from process_transcript.pdf_parser import PDFParser
from process_transcript.json_parser import JSONParser
from process_transcript.speaker_classifier import SpeakerClassifier

# Importer les endpoints de base de donn√©es
from api.db_endpoints import router as db_router

# Initialisation de l'API
app = FastAPI(
    title="aiko - LangGraph API",
    description="API pour le workflow d'analyse des besoins IA",
    version="1.0.0"
)

# Middleware de logging pour toutes les requ√™tes
import logging
from fastapi import Request
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Middleware pour logger toutes les requ√™tes"""
    start_time = time.time()
    
    # Logger la requ√™te entrante
    logger.info(f"üì• {request.method} {request.url.path} - Client: {request.client.host if request.client else 'unknown'}")
    
    try:
        response = await call_next(request)
        process_time = time.time() - start_time
        
        # Logger la r√©ponse
        logger.info(f"üì§ {request.method} {request.url.path} - Status: {response.status_code} - Time: {process_time:.3f}s")
        
        return response
    except Exception as e:
        process_time = time.time() - start_time
        logger.error(f"‚ùå {request.method} {request.url.path} - Error: {str(e)} - Time: {process_time:.3f}s")
        raise

# Inclure les endpoints de base de donn√©es
app.include_router(db_router)

# Logger tous les endpoints au d√©marrage
@app.on_event("startup")
async def startup_event():
    """Log tous les endpoints disponibles au d√©marrage"""
    routes = []
    for route in app.routes:
        if hasattr(route, 'methods') and hasattr(route, 'path'):
            routes.append(f"{', '.join(route.methods)} {route.path}")
    
    logger.info("=" * 80)
    logger.info("üöÄ API aiko d√©marr√©e - Endpoints disponibles:")
    for route in sorted(routes):
        logger.info(f"   {route}")
    logger.info("=" * 80)

# Stockage en m√©moire des workflows (en production, utiliser Redis ou DB)
workflows: Dict[str, Any] = {}
executive_workflows: Dict[str, Any] = {}  # Workflows Executive Summary
rappel_workflows: Dict[str, Any] = {}  # Workflows Rappel de la mission
atouts_workflows: Dict[str, Any] = {}  # Workflows Atouts de l'entreprise
value_chain_workflows: Dict[str, Any] = {}  # Workflows Cha√Æne de valeur
prerequis_evaluation_workflows: Dict[str, Any] = {}  # Workflows √âvaluation des pr√©requis
checkpointer = MemorySaver()

# Dossier temporaire pour les fichiers upload√©s
UPLOAD_DIR = Path("/tmp/aiko_uploads")
# Cr√©er le dossier avec les bonnes permissions
UPLOAD_DIR.mkdir(exist_ok=True, mode=0o755)
# V√©rifier que le dossier existe et est accessible en √©criture
if not UPLOAD_DIR.exists():
    raise RuntimeError(f"Impossible de cr√©er le dossier d'upload: {UPLOAD_DIR}")
if not os.access(UPLOAD_DIR, os.W_OK):
    raise RuntimeError(f"Le dossier d'upload n'est pas accessible en √©criture: {UPLOAD_DIR}")
print(f"‚úÖ Dossier d'upload initialis√©: {UPLOAD_DIR} (existe: {UPLOAD_DIR.exists()}, accessible: {os.access(UPLOAD_DIR, os.W_OK)})")


# ==================== MOD√àLES PYDANTIC ====================

class WorkflowInput(BaseModel):
    """Input pour d√©marrer un workflow"""
    workshop_document_ids: List[int] = []
    transcript_document_ids: List[int] = []
    company_name: Optional[str] = None
    company_url: Optional[str] = None
    company_description: Optional[str] = None
    validated_company_info: Optional[Dict[str, Any]] = None
    interviewer_names: Optional[List[str]] = None
    additional_context: Optional[str] = ""

class ValidationFeedback(BaseModel):
    """Feedback de validation utilisateur"""
    validated_needs: List[Dict[str, Any]]
    rejected_needs: List[Dict[str, Any]]
    user_feedback: str = ""
    user_action: str = "continue_needs"  # "continue_needs" ou "continue_to_use_cases"

class PreUseCaseContextInput(BaseModel):
    """Input pour le contexte additionnel avant g√©n√©ration des use cases"""
    use_case_additional_context: str = ""

class UseCaseValidationFeedback(BaseModel):
    """Feedback de validation des use cases"""
    validated_use_cases: List[Dict[str, Any]]
    rejected_use_cases: List[Dict[str, Any]]
    user_feedback: str = ""
    use_case_user_action: str = "finalize_use_cases"  # "continue_use_cases" ou "finalize_use_cases"

class ExecutiveSummaryInput(BaseModel):
    """Input pour d√©marrer un workflow Executive Summary"""
    transcript_document_ids: List[int] = []
    workshop_document_ids: List[int] = []
    company_name: str
    interviewer_note: str = ""
    validated_needs: Optional[List[Dict[str, Any]]] = None
    validated_use_cases: Optional[List[Dict[str, Any]]] = None


class RappelMissionInput(BaseModel):
    """Input pour d√©marrer un workflow de rappel de mission"""

    company_name: str
    validated_company_info: Optional[Dict[str, Any]] = None


class AtoutsEntrepriseInput(BaseModel):
    """Input pour d√©marrer un workflow d'extraction des atouts"""
    transcript_document_ids: List[int]  # IDs des documents transcripts dans la DB
    company_info: Dict[str, Any]
    interviewer_names: Optional[List[str]] = None
    atouts_additional_context: Optional[str] = ""
    validated_speakers: Optional[List[Dict[str, str]]] = None


class PreAtoutContextInput(BaseModel):
    """Input pour le contexte additionnel avant g√©n√©ration des atouts"""
    atouts_additional_context: str = ""


class AtoutsValidationFeedback(BaseModel):
    """Feedback de validation des atouts"""
    validated_atouts: List[Dict[str, Any]]
    rejected_atouts: List[Dict[str, Any]]
    user_feedback: str = ""
    atouts_user_action: str = "finalize_atouts"  # "continue_atouts" ou "finalize_atouts"


class ValueChainInput(BaseModel):
    """Input pour d√©marrer un workflow d'extraction de la cha√Æne de valeur"""
    transcript_document_ids: List[int]  # IDs des documents transcripts dans la DB
    company_info: Dict[str, Any]


class ValueChainValidationFeedback(BaseModel):
    """Feedback de validation de la cha√Æne de valeur"""
    validation_type: str  # "teams", "activities", "friction_points"
    validated_items: List[Dict[str, Any]]
    rejected_items: List[Dict[str, Any]]
    user_action: str  # "continue_teams", "continue_to_activities", "continue_activities", "continue_to_friction", "continue_friction", "finalize"


class PrerequisEvaluationInput(BaseModel):
    """Input pour d√©marrer un workflow d'√©valuation des pr√©requis"""
    transcript_document_ids: List[int]  # IDs des documents transcripts dans la DB
    company_info: Dict[str, Any]
    validated_use_cases: List[Dict[str, Any]]  # Cas d'usage valid√©s (obligatoire)
    comments: Optional[Dict[str, str]] = None  # Commentaires (comment_general, comment_1 √† comment_5)


class PrerequisValidationFeedback(BaseModel):
    """Feedback de validation des pr√©requis"""
    validated_prerequis: List[int]  # Liste des IDs des pr√©requis valid√©s (1 √† 5)
    regeneration_comment: str = ""  # Commentaire pour la r√©g√©n√©ration des pr√©requis non valid√©s
    comments: Optional[Dict[str, str]] = None  # Commentaires (comment_general, comment_1 √† comment_5)
    modified_evaluations: Optional[Dict[int, Dict[str, Any]]] = None  # Modifications des pr√©requis valid√©s (prerequis_id -> {note, evaluation_text})


class ExecutiveValidationFeedback(BaseModel):
    """Feedback de validation Executive Summary"""
    validation_type: str  # "challenges" ou "recommendations"
    validation_result: Dict[str, Any]

class ClassifySpeakersInput(BaseModel):
    """Input pour classifier les speakers d'un transcript"""
    file_path: str
    interviewer_names: Optional[List[str]] = None
    known_speakers: Optional[Dict[str, str]] = None  # speaker_name -> role pour r√©utilisation


class WordExtractInput(BaseModel):
    """Input pour extraire les donn√©es d'un fichier Word"""
    word_path: str
    force_llm: bool = False  # Si True, force l'extraction via LLM


class ParseTranscriptInput(BaseModel):
    """Input pour parser et sauvegarder un transcript"""
    file_path: str
    project_id: int
    file_name: str
    validated_speakers: List[Dict[str, Any]]  # Liste des speakers valid√©s
    metadata: Optional[Dict[str, Any]] = None


class ParseWorkshopInput(BaseModel):
    """Input pour parser et sauvegarder un workshop"""
    file_path: str
    project_id: int
    file_name: str
    metadata: Optional[Dict[str, Any]] = None


class ParseWordReportInput(BaseModel):
    """Input pour parser et sauvegarder un word report"""
    file_path: str
    project_id: int
    file_name: str
    metadata: Optional[Dict[str, Any]] = None


# ==================== ENDPOINTS ====================

@app.get("/")
async def root():
    """Health check"""
    return {
        "service": "aiko LangGraph API",
        "status": "running",
        "version": "1.0.0"
    }

@app.get("/health")
async def health_check():
    """Health check d√©taill√© avec liste des endpoints disponibles"""
    routes = []
    for route in app.routes:
        if hasattr(route, 'methods') and hasattr(route, 'path'):
            routes.append({
                "path": route.path,
                "methods": list(route.methods)
            })
    
    # V√©rifier sp√©cifiquement si l'endpoint classify-speakers existe
    classify_endpoint_exists = any(
        route.get("path") == "/transcripts/classify-speakers" 
        and "POST" in route.get("methods", [])
        for route in routes
    )
    
    return {
        "status": "healthy",
        "endpoints": routes,
        "classify_speakers_endpoint_exists": classify_endpoint_exists,
        "upload_dir": str(UPLOAD_DIR),
        "upload_dir_exists": UPLOAD_DIR.exists(),
        "upload_dir_writable": os.access(UPLOAD_DIR, os.W_OK) if UPLOAD_DIR.exists() else False
    }


@app.post("/files/upload")
async def upload_files(files: List[UploadFile] = File(...)):
    """
    Upload des fichiers et retourne les chemins locaux.
    
    Returns:
        {
            "file_paths": ["/tmp/aiko_uploads/uuid_filename.xlsx", ...],
            "file_types": {"workshop": [...], "transcript": [...]}
        }
    """
    try:
        # V√©rifier que le dossier d'upload existe et est accessible
        if not UPLOAD_DIR.exists():
            UPLOAD_DIR.mkdir(exist_ok=True, mode=0o755)
            print(f"‚ö†Ô∏è Dossier d'upload recr√©√©: {UPLOAD_DIR}")
        
        if not os.access(UPLOAD_DIR, os.W_OK):
            raise HTTPException(
                status_code=500, 
                detail=f"Le dossier d'upload n'est pas accessible en √©criture: {UPLOAD_DIR}"
            )
        
        file_paths = []
        workshop_files = []
        transcript_files = []
        
        for file in files:
            # G√©n√©rer un nom unique
            file_id = str(uuid.uuid4())
            file_extension = Path(file.filename).suffix
            file_path = UPLOAD_DIR / f"{file_id}_{file.filename}"
            
            # Sauvegarder le fichier
            content = await file.read()
            content_size = len(content)
            
            # √âcrire le fichier
            with open(file_path, "wb") as f:
                f.write(content)
            
            # V√©rifier que le fichier a bien √©t√© √©crit
            if not file_path.exists():
                raise HTTPException(
                    status_code=500,
                    detail=f"Le fichier n'a pas pu √™tre sauvegard√©: {file_path}"
                )
            
            # V√©rifier la taille du fichier √©crit
            written_size = file_path.stat().st_size
            if written_size != content_size:
                raise HTTPException(
                    status_code=500,
                    detail=f"Taille du fichier incorrecte: attendu {content_size} octets, obtenu {written_size} octets"
                )
            
            print(f"‚úÖ Fichier sauvegard√©: {file_path} ({written_size} octets)")
            
            file_paths.append(str(file_path))
            
            # Classifier par type
            if file_extension == ".xlsx":
                workshop_files.append(str(file_path))
            elif file_extension in [".pdf", ".json"]:
                transcript_files.append(str(file_path))
            elif file_extension == ".docx":
                # Fichier Word pour Executive Summary
                workshop_files.append(str(file_path))  # Pour l'instant, on le met dans workshop_files
        
        print(f"‚úÖ {len(file_paths)} fichier(s) upload√©(s) avec succ√®s dans {UPLOAD_DIR}")
        
        return {
            "file_paths": file_paths,
            "file_types": {
                "workshop": workshop_files,
                "transcript": transcript_files
            },
            "count": len(file_paths)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"‚ùå Erreur lors de l'upload: {str(e)}")
        import traceback
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur upload: {str(e)}")


@app.post("/transcripts/classify-speakers")
async def classify_speakers(input_data: ClassifySpeakersInput):
    """
    Classe les speakers d'un transcript et extrait leurs r√¥les
    
    Args:
        input_data: Contient file_path, interviewer_names (optionnel), et known_speakers (optionnel)
    
    Returns:
        {
            "speakers": [
                {"name": "...", "role": "...", "level": "direction"|"m√©tier"|"inconnu"|None, "is_interviewer": bool},
                ...
            ]
        }
    """
    logger.info(f"üîç [classify-speakers] D√©but de la classification - file_path: {input_data.file_path}")
    try:
        file_path = input_data.file_path
        
        # DEBUG: V√©rifier si le chemin contient aiko_uploads_local au lieu de aiko_uploads
        if "aiko_uploads_local" in file_path:
            logger.warning(f"‚ö†Ô∏è [classify-speakers] Chemin contient 'aiko_uploads_local', correction automatique")
            file_path = file_path.replace("/tmp/aiko_uploads_local", "/tmp/aiko_uploads")
            logger.info(f"üîß [classify-speakers] Chemin corrig√©: {file_path}")
        
        # V√©rifier que le fichier existe
        file_path_obj = Path(file_path)
        if not file_path_obj.exists():
            logger.error(f"‚ùå [classify-speakers] Fichier non trouv√©: {file_path}")
            # Essayer de trouver le fichier dans aiko_uploads si on cherchait dans aiko_uploads_local
            if "aiko_uploads_local" in str(input_data.file_path):
                alternative_path = str(input_data.file_path).replace("/tmp/aiko_uploads_local", "/tmp/aiko_uploads")
                logger.info(f"üîç [classify-speakers] Tentative avec chemin alternatif: {alternative_path}")
                if Path(alternative_path).exists():
                    file_path = alternative_path
                    file_path_obj = Path(file_path)
                    logger.info(f"‚úÖ [classify-speakers] Fichier trouv√© avec chemin alternatif")
                else:
                    raise HTTPException(status_code=404, detail=f"Fichier non trouv√©: {file_path}")
            else:
                raise HTTPException(status_code=404, detail=f"Fichier non trouv√©: {file_path}")
        logger.info(f"‚úÖ [classify-speakers] Fichier trouv√©: {file_path} ({file_path_obj.stat().st_size} octets)")
        
        # Initialiser les parsers et le classificateur
        pdf_parser = PDFParser()
        json_parser = JSONParser()
        
        interviewer_names = input_data.interviewer_names or ["Christella Umuhoza", "Adrien Fabry"]
        speaker_classifier = SpeakerClassifier(
            api_key=os.getenv("OPENAI_API_KEY"),
            interviewer_names=interviewer_names
        )
        
        # Parser le transcript selon son type
        file_extension = Path(file_path).suffix.lower()
        if file_extension == '.json':
            interventions = json_parser.parse_transcript(file_path)
        elif file_extension == '.pdf':
            interventions = pdf_parser.parse_transcript(file_path)
        else:
            raise HTTPException(status_code=400, detail=f"Format de fichier non support√©: {file_extension}")
        
        if not interventions:
            return {"speakers": []}
        
        # Identifier les interviewers (par matching de noms)
        interviewer_names_set = speaker_classifier._identify_interviewers(interventions)
        
        # Construire le dictionnaire de r√¥les connus
        known_roles = input_data.known_speakers or {}
        
        # Pour JSON : extraire directement les speaker_name uniques et utiliser extract_roles_for_json_speakers
        # Pour PDF : utiliser identify_and_extract_speakers_with_roles (identification + extraction r√¥les)
        if file_extension == '.json':
            # Extraire directement les speaker_name uniques du JSON
            json_speakers = list(set(
                interv.get("speaker", "") 
                for interv in interventions 
                if interv.get("speaker")
            ))
            
            logger.info(f"üîç [classify-speakers] Extraction r√¥les pour {len(json_speakers)} speakers JSON")
            
            # Utiliser la nouvelle fonction qui n'identifie pas les speakers (d√©j√† fait par le JSON)
            speakers_list = speaker_classifier.extract_roles_for_json_speakers(
                json_speakers=json_speakers,
                interventions=interventions,
                interviewer_names_set=interviewer_names_set,
                known_roles=known_roles
            )
        else:
            # Pour PDF : extraire TOUS les speakers uniques et identifier les vrais speakers
            all_speakers = list(set(
                interv.get("speaker", "") 
                for interv in interventions 
                if interv.get("speaker")
            ))
            
            logger.info(f"üîç [classify-speakers] Classification de {len(all_speakers)} speakers uniques (PDF)")
            
            # Utiliser la fonction qui identifie ET extrait les r√¥les
            speakers_list = speaker_classifier.identify_and_extract_speakers_with_roles(
                all_speakers=all_speakers,
                interventions=interventions,
                interviewer_names_set=interviewer_names_set,
                known_roles=known_roles
            )
        
        logger.info(f"‚úÖ [classify-speakers] Classification termin√©e - {len(speakers_list)} speakers identifi√©s")
        return {"speakers": speakers_list}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [classify-speakers] Erreur lors de la classification: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur classification: {str(e)}")


@app.post("/word/extract")
async def extract_word_data(input_data: WordExtractInput):
    """
    Extrait les besoins et cas d'usage depuis un fichier Word.
    
    Args:
        input_data: Contient word_path (chemin vers le fichier .docx) et force_llm (optionnel)
    
    Returns:
        {
            "final_needs": [
                {"titre": "...", "description": "..."},
                ...
            ],
            "final_use_cases": [
                {"titre": "...", "description": "...", "famille": "..."},
                ...
            ],
            "extraction_method": "structured" | "llm_fallback" | "llm_forced"
        }
    """
    try:
        word_path = input_data.word_path
        force_llm = input_data.force_llm
        
        # V√©rifier que le fichier existe
        if not Path(word_path).exists():
            raise HTTPException(status_code=404, detail=f"Fichier non trouv√©: {word_path}")
        
        # Importer et utiliser WordReportExtractor
        from executive_summary.word_report_extractor import WordReportExtractor
        
        extractor = WordReportExtractor()
        extracted_data = extractor.extract_from_word(word_path, force_llm=force_llm)
        
        return extracted_data
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [word/extract] Erreur lors de l'extraction: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur extraction: {str(e)}")


@app.post("/documents/parse-transcript")
async def parse_and_save_transcript(input_data: ParseTranscriptInput):
    """
    Parse et sauvegarde un transcript dans la base de donn√©es.
    
    Args:
        input_data: Contient file_path, project_id, file_name, validated_speakers, metadata
    
    Returns:
        {"document_id": int}
    """
    try:
        logger.info(f"üîç [parse-transcript] D√©but du parsing - file_path: {input_data.file_path}")
        
        # Importer DocumentParserService
        from database.document_parser_service import DocumentParserService
        
        parser_service = DocumentParserService()
        document_id = parser_service.parse_and_save_transcript(
            file_path=input_data.file_path,
            project_id=input_data.project_id,
            file_name=input_data.file_name,
            validated_speakers=input_data.validated_speakers,
            metadata=input_data.metadata
        )
        
        logger.info(f"‚úÖ [parse-transcript] Document sauvegard√© avec ID: {document_id}")
        return {"document_id": document_id}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [parse-transcript] Erreur lors du parsing: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur parsing: {str(e)}")


@app.post("/documents/parse-workshop")
async def parse_and_save_workshop(input_data: ParseWorkshopInput):
    """
    Parse et sauvegarde un workshop dans la base de donn√©es.
    
    Args:
        input_data: Contient file_path, project_id, file_name, metadata
    
    Returns:
        {"document_id": int}
    """
    try:
        logger.info(f"üîç [parse-workshop] D√©but du parsing - file_path: {input_data.file_path}")
        
        # Importer DocumentParserService
        from database.document_parser_service import DocumentParserService
        
        parser_service = DocumentParserService()
        document_id = parser_service.parse_and_save_workshop(
            file_path=input_data.file_path,
            project_id=input_data.project_id,
            file_name=input_data.file_name,
            metadata=input_data.metadata
        )
        
        logger.info(f"‚úÖ [parse-workshop] Document sauvegard√© avec ID: {document_id}")
        return {"document_id": document_id}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [parse-workshop] Erreur lors du parsing: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur parsing: {str(e)}")


@app.post("/documents/parse-word-report")
async def parse_and_save_word_report(input_data: ParseWordReportInput):
    """
    Parse et sauvegarde un word report dans la base de donn√©es.
    
    Args:
        input_data: Contient file_path, project_id, file_name, metadata
    
    Returns:
        {"document_id": int}
    """
    try:
        logger.info(f"üîç [parse-word-report] D√©but du parsing - file_path: {input_data.file_path}")
        
        # Importer DocumentParserService
        from database.document_parser_service import DocumentParserService
        
        parser_service = DocumentParserService()
        document_id = parser_service.parse_and_save_word_report(
            file_path=input_data.file_path,
            project_id=input_data.project_id,
            file_name=input_data.file_name,
            metadata=input_data.metadata
        )
        
        logger.info(f"‚úÖ [parse-word-report] Document sauvegard√© avec ID: {document_id}")
        return {"document_id": document_id}
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå [parse-word-report] Erreur lors du parsing: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"Erreur parsing: {str(e)}")


@app.post("/threads/{thread_id}/runs")
async def create_run(thread_id: str, workflow_input: WorkflowInput):
    """
    D√©marre ou reprend un workflow.
    
    Args:
        thread_id: ID du thread (UUID)
        workflow_input: Fichiers et param√®tres du workflow
    
    Returns:
        {
            "run_id": "uuid",
            "thread_id": "uuid",
            "status": "running"
        }
    """
    try:
        # Cr√©er ou r√©cup√©rer le workflow
        if thread_id not in workflows:
            # Nouveau workflow
            api_key = os.getenv("OPENAI_API_KEY")
            # V√©rifier si DEV_MODE est activ√©
            dev_mode = os.getenv("DEV_MODE", "0") == "1"
            workflow = NeedAnalysisWorkflow(
                api_key=api_key,
                dev_mode=dev_mode  # Activer dev_mode si DEV_MODE=1
            )
            workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created"
            }
        
        workflow_data = workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        # Lancer le workflow
        print(f"\nüöÄ [API] D√©marrage du workflow pour thread {thread_id}")
        print(f"üìÅ Workshop document IDs: {workflow_input.workshop_document_ids}")
        print(f"üìÅ Transcript document IDs: {workflow_input.transcript_document_ids}")
        print(f"üè¢ Company: {workflow_input.company_name}")
        if workflow_input.company_url:
            print(f"üåê Company URL: {workflow_input.company_url}")
        if workflow_input.company_description:
            print(f"üìÑ Company Description: {workflow_input.company_description}")
        print(f"üë• Interviewers: {workflow_input.interviewer_names}")
        print(f"üìù Additional context: {len(workflow_input.additional_context or '')} caract√®res")
        
        # Construire company_info avec tous les champs disponibles
        # Si validated_company_info est fourni, l'utiliser directement
        if workflow_input.validated_company_info:
            company_info = workflow_input.validated_company_info
        else:
            # Sinon, construire √† partir des champs individuels (r√©trocompatibilit√©)
            company_info = {}
            if workflow_input.company_name:
                company_info["company_name"] = workflow_input.company_name
            if workflow_input.company_url:
                company_info["company_url"] = workflow_input.company_url
            if workflow_input.company_description:
                company_info["company_description"] = workflow_input.company_description
        
        # Ex√©cuter le workflow (mode asynchrone g√©r√© par LangGraph)
        result = workflow.run(
            workshop_document_ids=workflow_input.workshop_document_ids,
            transcript_document_ids=workflow_input.transcript_document_ids,
            company_info=company_info,
            interviewer_names=workflow_input.interviewer_names,
            thread_id=thread_id,
            additional_context=workflow_input.additional_context or ""
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        workflow_data["status"] = "completed" if result.get("success") else "paused"
        
        run_id = str(uuid.uuid4())
        
        return {
            "run_id": run_id,
            "thread_id": thread_id,
            "status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow: {str(e)}")


@app.get("/threads/{thread_id}/state")
async def get_state(thread_id: str):
    """
    R√©cup√®re l'√©tat actuel du workflow.
    Utilise le snapshot LangGraph pour d√©terminer le vrai prochain n≈ìud.
    
    Returns:
        {
            "thread_id": "uuid",
            "status": "running" | "paused" | "completed",
            "values": {...},  # √âtat complet
            "next": ["node_name"] | []  # Prochain n≈ìud ou vide si termin√©
        }
    """
    if thread_id not in workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # R√©cup√©rer l'√©tat actuel depuis le checkpointer LangGraph
    config = {"configurable": {"thread_id": thread_id}}
    snapshot = workflow.graph.get_state(config)
    
    if snapshot and snapshot.values:
        state = snapshot.values
        workflow_data["state"] = state
        
        # D√©terminer le prochain n≈ìud depuis le snapshot LangGraph
        next_nodes = []
        if snapshot.next:
            if isinstance(snapshot.next, (list, tuple)):
                next_nodes = list(snapshot.next)
            else:
                next_nodes = [snapshot.next]
        
        # Mettre √† jour le statut en fonction du prochain n≈ìud
        if "human_validation" in next_nodes:
            workflow_data["status"] = "paused"
        elif "pre_use_case_interrupt" in next_nodes:
            workflow_data["status"] = "paused"
        elif "validate_use_cases" in next_nodes:
            workflow_data["status"] = "paused"
        elif len(next_nodes) == 0:
            workflow_data["status"] = "completed"
        else:
            workflow_data["status"] = "running"
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "values": state,
            "next": tuple(next_nodes) if next_nodes else []
        }
    else:
        # Fallback si pas de snapshot
        state = workflow_data.get("state", {})
        return {
            "thread_id": thread_id,
            "status": workflow_data.get("status", "paused"),
            "values": state,
            "next": []
        }


@app.post("/threads/{thread_id}/validation")
async def send_validation(thread_id: str, feedback: ValidationFeedback):
    """
    Envoie le feedback de validation des besoins et reprend le workflow.
    
    Args:
        thread_id: ID du thread
        feedback: Feedback utilisateur
    
    Returns:
        {
            "status": "resumed",
            "thread_id": "uuid"
        }
    """
    if thread_id not in workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du feedback de validation pour thread {thread_id}")
        print(f"‚úÖ Valid√©s: {len(feedback.validated_needs)}")
        print(f"‚ùå Rejet√©s: {len(feedback.rejected_needs)}")
        print(f"üéØ Action utilisateur: {feedback.user_action}")
        
        # Reprendre le workflow avec le feedback
        result = workflow.resume_workflow_with_feedback(
            validated_needs=feedback.validated_needs,
            rejected_needs=feedback.rejected_needs,
            user_feedback=feedback.user_feedback,
            user_action=feedback.user_action,
            thread_id=thread_id
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # R√©cup√©rer le snapshot LangGraph pour d√©terminer le vrai statut
        config = {"configurable": {"thread_id": thread_id}}
        snapshot = workflow.graph.get_state(config)
        
        if snapshot and snapshot.next:
            next_nodes = list(snapshot.next) if isinstance(snapshot.next, (list, tuple)) else [snapshot.next]
            
            # Mettre √† jour le statut en fonction du prochain n≈ìud r√©el
            if "pre_use_case_interrupt" in next_nodes:
                workflow_data["status"] = "paused"  # Va s'arr√™ter √† pre_use_case_interrupt
            elif "human_validation" in next_nodes:
                workflow_data["status"] = "paused"  # Va s'arr√™ter √† human_validation
            elif len(next_nodes) == 0:
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "running"
        else:
            # Fallback : d√©terminer le statut selon l'action utilisateur
            if feedback.user_action == "continue_to_use_cases":
                workflow_data["status"] = "paused"  # Va s'arr√™ter √† pre_use_case_interrupt
            else:
                workflow_data["status"] = "paused"  # Va continuer avec analyze_needs
        
        return {
            "status": "resumed",
            "thread_id": thread_id,
            "workflow_status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur reprise workflow: {str(e)}")


@app.post("/threads/{thread_id}/pre-use-case-context")
async def send_pre_use_case_context(thread_id: str, context_input: PreUseCaseContextInput):
    """
    Envoie le contexte additionnel avant la g√©n√©ration des use cases et reprend le workflow.
    
    Args:
        thread_id: ID du thread
        context_input: Contexte additionnel
    
    Returns:
        {
            "status": "resumed",
            "thread_id": "uuid"
        }
    """
    if thread_id not in workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du contexte additionnel pour thread {thread_id}")
        print(f"üí° Contexte: {len(context_input.use_case_additional_context)} caract√®res")
        
        # Reprendre le workflow avec le contexte
        result = workflow.resume_pre_use_case_interrupt_with_context(
            use_case_additional_context=context_input.use_case_additional_context,
            thread_id=thread_id
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        workflow_data["status"] = "running"  # Le workflow va g√©n√©rer les use cases
        
        return {
            "status": "resumed",
            "thread_id": thread_id,
            "workflow_status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur reprise workflow: {str(e)}")


@app.post("/threads/{thread_id}/use-case-validation")
async def send_use_case_validation(thread_id: str, feedback: UseCaseValidationFeedback):
    """
    Envoie le feedback de validation des use cases et reprend le workflow.
    
    Args:
        thread_id: ID du thread
        feedback: Feedback utilisateur
    
    Returns:
        {
            "status": "completed",
            "thread_id": "uuid"
        }
    """
    if thread_id not in workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du feedback use cases pour thread {thread_id}")
        print(f"‚úÖ Cas d'usage valid√©s: {len(feedback.validated_use_cases)}")
        print(f"üéØ Action: {feedback.use_case_user_action}")
        
        # Reprendre le workflow avec le feedback
        result = workflow.resume_use_case_workflow_with_feedback(
            validated_use_cases=feedback.validated_use_cases,
            rejected_use_cases=feedback.rejected_use_cases,
            user_feedback=feedback.user_feedback,
            use_case_user_action=feedback.use_case_user_action,
            thread_id=thread_id
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # D√©terminer le statut selon l'action utilisateur
        if feedback.use_case_user_action == "finalize_use_cases":
            workflow_data["status"] = "completed"
        else:
            workflow_data["status"] = "paused"  # Va continuer avec analyze_use_cases
        
        return {
            "status": workflow_data["status"],
            "thread_id": thread_id,
            "final_results": result,
            "success": result.get("success", False),
            "workflow_status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur reprise workflow: {str(e)}")


@app.post("/rappel-mission/threads/{thread_id}/runs")
async def create_rappel_mission_run(thread_id: str, mission_input: RappelMissionInput):
    """D√©marre un workflow d√©di√© au rappel de la mission."""

    try:
        if thread_id not in rappel_workflows:
            workflow = RappelMissionWorkflow()
            rappel_workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created",
            }

        workflow_data = rappel_workflows[thread_id]
        workflow = workflow_data["workflow"]

        result = workflow.run(
            company_name=mission_input.company_name,
            validated_company_info=mission_input.validated_company_info,
            thread_id=thread_id
        )

        workflow_data["state"] = result
        workflow_data["status"] = "completed" if result.get("success") else "error"

        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "result": result,
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur rappel mission: {str(e)}")


@app.get("/rappel-mission/threads/{thread_id}/state")
async def get_rappel_mission_state(thread_id: str):
    """R√©cup√®re l'√©tat courant du workflow rappel de mission."""

    if thread_id not in rappel_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")

    return {
        "thread_id": thread_id,
        "status": rappel_workflows[thread_id]["status"],
        "state": rappel_workflows[thread_id]["state"],
    }


@app.post("/atouts-entreprise/threads/{thread_id}/runs")
async def create_atouts_run(thread_id: str, atouts_input: AtoutsEntrepriseInput):
    """D√©marre un workflow d'extraction des atouts de l'entreprise"""
    try:
        if thread_id not in atouts_workflows:
            workflow = AtoutsWorkflow(
                interviewer_names=atouts_input.interviewer_names,
                checkpointer=checkpointer
            )
            atouts_workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created"
            }
        
        workflow_data = atouts_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüöÄ [API] D√©marrage workflow Atouts pour thread {thread_id}")
        print(f"üìÅ Documents: {len(atouts_input.transcript_document_ids)}")
        print(f"üè¢ Entreprise: {atouts_input.company_info.get('nom', 'N/A')}")
        print(f"üìù Contexte additionnel: {len(atouts_input.atouts_additional_context)} caract√®res")
        
        # Ex√©cuter le workflow avec le contexte additionnel
        result = workflow.run(
            transcript_document_ids=atouts_input.transcript_document_ids,
            company_info=atouts_input.company_info,
            thread_id=thread_id,
            atouts_additional_context=atouts_input.atouts_additional_context,
            validated_speakers=atouts_input.validated_speakers
        )
        
        workflow_data["state"] = result
        workflow_data["status"] = "paused" if result.get("atouts_workflow_paused") else ("completed" if result.get("success") else "error")
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "result": result
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur Atouts: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow atouts: {str(e)}")


@app.get("/atouts-entreprise/threads/{thread_id}/state")
async def get_atouts_state(thread_id: str):
    """
    R√©cup√®re l'√©tat actuel du workflow Atouts.
    Utilise le snapshot LangGraph pour d√©terminer le vrai prochain n≈ìud.
    """
    if thread_id not in atouts_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = atouts_workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # R√©cup√©rer l'√©tat actuel depuis le checkpointer LangGraph
    config = {"configurable": {"thread_id": thread_id}}
    snapshot = workflow.graph.get_state(config)
    
    if snapshot and snapshot.values:
        state = snapshot.values
        workflow_data["state"] = state
        
        # D√©terminer le prochain n≈ìud depuis le snapshot LangGraph
        next_nodes = []
        if snapshot.next:
            if isinstance(snapshot.next, (list, tuple)):
                next_nodes = list(snapshot.next)
            else:
                next_nodes = [snapshot.next]
        
        # Mettre √† jour le statut en fonction du prochain n≈ìud
        if "pre_atout_interrupt" in next_nodes:
            workflow_data["status"] = "paused"
        elif "validate_atouts" in next_nodes:
            workflow_data["status"] = "paused"
        elif len(next_nodes) == 0:
            workflow_data["status"] = "completed"
        else:
            workflow_data["status"] = "running"
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "values": state,
            "next": tuple(next_nodes) if next_nodes else []
        }
    else:
        # Fallback si pas de snapshot
        state = workflow_data.get("state", {})
        return {
            "thread_id": thread_id,
            "status": workflow_data.get("status", "paused"),
            "values": state,
            "next": []
        }


@app.post("/atouts-entreprise/threads/{thread_id}/validate")
async def send_atouts_validation(thread_id: str, feedback: AtoutsValidationFeedback):
    """
    Envoie le feedback de validation des atouts et reprend le workflow.
    """
    if thread_id not in atouts_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = atouts_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du feedback de validation atouts pour thread {thread_id}")
        print(f"‚úÖ Valid√©s: {len(feedback.validated_atouts)}")
        print(f"‚ùå Rejet√©s: {len(feedback.rejected_atouts)}")
        print(f"üéØ Action utilisateur: {feedback.atouts_user_action}")
        
        # Reprendre le workflow avec le feedback
        result = workflow.resume_workflow_with_validation(
            validated_atouts=feedback.validated_atouts,
            rejected_atouts=feedback.rejected_atouts,
            user_feedback=feedback.user_feedback,
            atouts_user_action=feedback.atouts_user_action,
            thread_id=thread_id
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # R√©cup√©rer le snapshot LangGraph pour d√©terminer le vrai statut
        config = {"configurable": {"thread_id": thread_id}}
        snapshot = workflow.graph.get_state(config)
        
        if snapshot and snapshot.next:
            next_nodes = list(snapshot.next) if isinstance(snapshot.next, (list, tuple)) else [snapshot.next]
            
            if "validate_atouts" in next_nodes:
                workflow_data["status"] = "paused"
            elif len(next_nodes) == 0:
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "running"
        else:
            if result.get("success"):
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "paused"
        
        return {
            "status": "resumed",
            "thread_id": thread_id,
            "workflow_status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur reprise workflow: {str(e)}")


# ==================== ENDPOINTS CHA√éNE DE VALEUR ====================

@app.post("/value-chain/threads/{thread_id}/runs")
async def create_value_chain_run(thread_id: str, value_chain_input: ValueChainInput):
    """D√©marre un workflow d'extraction de la cha√Æne de valeur"""
    try:
        if thread_id not in value_chain_workflows:
            workflow = ValueChainWorkflow(checkpointer=checkpointer)
            value_chain_workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created"
            }
        
        workflow_data = value_chain_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüöÄ [API] D√©marrage workflow Cha√Æne de valeur pour thread {thread_id}")
        print(f"üìÅ Documents: {len(value_chain_input.transcript_document_ids)}")
        print(f"üè¢ Entreprise: {value_chain_input.company_info.get('nom', 'N/A')}")
        
        # Ex√©cuter le workflow
        result = workflow.run(
            transcript_document_ids=value_chain_input.transcript_document_ids,
            company_info=value_chain_input.company_info,
            thread_id=thread_id
        )
        
        workflow_data["state"] = result
        workflow_data["status"] = "paused" if result.get("workflow_paused") else ("completed" if result.get("success") else "error")
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "result": result
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur Cha√Æne de valeur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow cha√Æne de valeur: {str(e)}")


@app.get("/value-chain/threads/{thread_id}/state")
async def get_value_chain_state(thread_id: str):
    """
    R√©cup√®re l'√©tat actuel du workflow Cha√Æne de valeur.
    Utilise le snapshot LangGraph pour d√©terminer le vrai prochain n≈ìud.
    """
    # Si le thread n'existe pas encore dans le dictionnaire, v√©rifier s'il existe dans le checkpointer
    if thread_id not in value_chain_workflows:
        # V√©rifier si le thread existe dans le checkpointer LangGraph
        try:
            # Cr√©er un workflow temporaire pour v√©rifier
            temp_workflow = ValueChainWorkflow(checkpointer=checkpointer)
            config = {"configurable": {"thread_id": thread_id}}
            snapshot = temp_workflow.graph.get_state(config)
            
            if snapshot and snapshot.values:
                # Le thread existe dans le checkpointer, initialiser le workflow
                workflow = ValueChainWorkflow(checkpointer=checkpointer)
                value_chain_workflows[thread_id] = {
                    "workflow": workflow,
                    "state": snapshot.values,
                    "status": "paused" if snapshot.next else "completed"
                }
            else:
                # Le thread n'existe nulle part
                raise HTTPException(status_code=404, detail="Thread non trouv√©")
        except HTTPException:
            raise
        except Exception as e:
            # Si erreur lors de la v√©rification, retourner 404
            raise HTTPException(status_code=404, detail=f"Thread non trouv√©: {str(e)}")
    
    workflow_data = value_chain_workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # R√©cup√©rer l'√©tat actuel depuis le checkpointer LangGraph
    config = {"configurable": {"thread_id": thread_id}}
    snapshot = workflow.graph.get_state(config)
    
    if snapshot and snapshot.values:
        state = snapshot.values
        workflow_data["state"] = state
        
        # D√©terminer le prochain n≈ìud depuis le snapshot LangGraph
        next_nodes = []
        if snapshot.next:
            if isinstance(snapshot.next, (list, tuple)):
                next_nodes = list(snapshot.next)
            else:
                next_nodes = [snapshot.next]
        
        # Mettre √† jour le statut en fonction du prochain n≈ìud
        if any(node in next_nodes for node in ["validate_teams", "validate_activities", "validate_friction_points"]):
            workflow_data["status"] = "paused"
        elif len(next_nodes) == 0:
            workflow_data["status"] = "completed"
        else:
            workflow_data["status"] = "running"
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "values": state,
            "next": tuple(next_nodes) if next_nodes else []
        }
    else:
        # Fallback si pas de snapshot
        state = workflow_data.get("state", {})
        return {
            "thread_id": thread_id,
            "status": workflow_data.get("status", "paused"),
            "values": state,
            "next": []
        }


@app.post("/value-chain/threads/{thread_id}/validate")
async def send_value_chain_validation(thread_id: str, feedback: ValueChainValidationFeedback):
    """
    Envoie le feedback de validation de la cha√Æne de valeur et reprend le workflow.
    """
    if thread_id not in value_chain_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = value_chain_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du feedback de validation cha√Æne de valeur pour thread {thread_id}")
        print(f"üìã Type de validation: {feedback.validation_type}")
        print(f"‚úÖ Valid√©s: {len(feedback.validated_items)}")
        print(f"‚ùå Rejet√©s: {len(feedback.rejected_items)}")
        print(f"üéØ Action utilisateur: {feedback.user_action}")
        
        # Reprendre le workflow avec le feedback
        result = workflow.resume_workflow_with_validation(
            validation_type=feedback.validation_type,
            validated_items=feedback.validated_items,
            rejected_items=feedback.rejected_items,
            user_action=feedback.user_action,
            thread_id=thread_id
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # R√©cup√©rer le snapshot LangGraph pour d√©terminer le vrai statut
        config = {"configurable": {"thread_id": thread_id}}
        snapshot = workflow.graph.get_state(config)
        
        if snapshot and snapshot.next:
            next_nodes = list(snapshot.next) if isinstance(snapshot.next, (list, tuple)) else [snapshot.next]
            
            if any(node in next_nodes for node in ["validate_teams", "validate_activities", "validate_friction_points"]):
                workflow_data["status"] = "paused"
            elif len(next_nodes) == 0:
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "running"
        else:
            if result.get("success"):
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "paused"
        
        return {
            "status": "resumed",
            "thread_id": thread_id,
            "workflow_status": workflow_data["status"],
            "result": result
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur reprise workflow: {str(e)}")


@app.delete("/threads/{thread_id}")
async def delete_thread(thread_id: str):
    """
    Supprime un thread et nettoie les ressources.
    
    Returns:
        {"status": "deleted"}
    """
    deleted = False

    if thread_id in workflows:
        del workflows[thread_id]
        deleted = True

    if thread_id in executive_workflows:
        del executive_workflows[thread_id]
        deleted = True

    if thread_id in rappel_workflows:
        del rappel_workflows[thread_id]
        deleted = True

    if thread_id in atouts_workflows:
        del atouts_workflows[thread_id]
        deleted = True

    if thread_id in value_chain_workflows:
        del value_chain_workflows[thread_id]
        deleted = True

    if thread_id in prerequis_evaluation_workflows:
        del prerequis_evaluation_workflows[thread_id]
        deleted = True

    if deleted:
        return {"status": "deleted", "thread_id": thread_id}

    raise HTTPException(status_code=404, detail="Thread non trouv√©")


# ==================== ENDPOINTS √âVALUATION PR√âREQUIS ====================

@app.post("/prerequis-evaluation/threads/{thread_id}/runs")
async def create_prerequis_evaluation_run(thread_id: str, prerequis_input: PrerequisEvaluationInput):
    """D√©marre un workflow d'√©valuation des pr√©requis"""
    try:
        if thread_id not in prerequis_evaluation_workflows:
            workflow = PrerequisEvaluationWorkflow(checkpointer=checkpointer)
            prerequis_evaluation_workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created"
            }
        
        workflow_data = prerequis_evaluation_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüöÄ [API] D√©marrage workflow √âvaluation pr√©requis pour thread {thread_id}")
        print(f"üìÅ Documents: {len(prerequis_input.transcript_document_ids)}")
        print(f"üè¢ Entreprise: {prerequis_input.company_info.get('nom', 'N/A')}")
        print(f"üìã Cas d'usage valid√©s: {len(prerequis_input.validated_use_cases)}")
        
        # Ex√©cuter le workflow
        result = workflow.run(
            transcript_document_ids=prerequis_input.transcript_document_ids,
            company_info=prerequis_input.company_info,
            validated_use_cases=prerequis_input.validated_use_cases,
            thread_id=thread_id,
            comments=prerequis_input.comments
        )
        
        workflow_data["state"] = result
        
        # V√©rifier si on est en attente de validation
        if result.get("validation_pending", False):
            workflow_data["status"] = "validation_pending"
        else:
            workflow_data["status"] = "completed" if result.get("success") else "error"
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "result": result,
            "validation_pending": result.get("validation_pending", False)
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur √âvaluation pr√©requis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow √©valuation pr√©requis: {str(e)}")


@app.get("/prerequis-evaluation/threads/{thread_id}/state")
async def get_prerequis_evaluation_state(thread_id: str):
    """
    R√©cup√®re l'√©tat actuel du workflow d'√©valuation des pr√©requis
    
    Args:
        thread_id: ID du thread
        
    Returns:
        √âtat du workflow
    """
    if thread_id not in prerequis_evaluation_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = prerequis_evaluation_workflows[thread_id]
    
    return {
        "thread_id": thread_id,
        "status": workflow_data["status"],
        "state": workflow_data["state"],
        "result": workflow_data["state"],  # Alias pour compatibilit√© avec Streamlit
        "validation_pending": workflow_data["state"].get("validation_pending", False) if workflow_data["state"] else False
    }


@app.post("/prerequis-evaluation/threads/{thread_id}/validate")
async def send_prerequis_validation(thread_id: str, feedback: PrerequisValidationFeedback):
    """
    Envoie le feedback de validation des pr√©requis et reprend le workflow.
    
    Args:
        thread_id: ID du thread
        feedback: Feedback utilisateur avec les pr√©requis valid√©s et le commentaire de r√©g√©n√©ration
    
    Returns:
        {
            "status": "resumed",
            "thread_id": "uuid",
            "result": {...}
        }
    """
    if thread_id not in prerequis_evaluation_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = prerequis_evaluation_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüìù [API] R√©ception du feedback de validation pour thread {thread_id}")
        print(f"‚úÖ Valid√©s: {feedback.validated_prerequis}")
        print(f"üí¨ Commentaire r√©g√©n√©ration: {feedback.regeneration_comment[:50]}..." if feedback.regeneration_comment else "üí¨ Pas de commentaire")
        
        # Reprendre le workflow avec le feedback
        result = workflow.resume_workflow_with_validation(
            validated_prerequis=feedback.validated_prerequis,
            regeneration_comment=feedback.regeneration_comment,
            thread_id=thread_id,
            modified_evaluations=feedback.modified_evaluations
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # V√©rifier si on est encore en attente de validation (nouvelle boucle)
        if result.get("validation_pending", False):
            workflow_data["status"] = "validation_pending"
        else:
            workflow_data["status"] = "completed" if result.get("success") else "error"
        
        return {
            "thread_id": thread_id,
            "status": workflow_data["status"],
            "result": result,
            "validation_pending": result.get("validation_pending", False)
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur validation pr√©requis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur validation pr√©requis: {str(e)}")


# ==================== D√âMARRAGE ====================

if __name__ == "__main__":
    print("üöÄ D√©marrage de l'API LangGraph aiko...")
    print("üìç URL: http://localhost:2025")
    print("üìñ Documentation: http://localhost:2025/docs")
    print("‚ÑπÔ∏è  LangGraph Studio utilise le port 2024")
    
# ==================== ENDPOINTS EXECUTIVE SUMMARY ====================

@app.post("/executive-summary/threads/{thread_id}/runs")
async def create_executive_run(thread_id: str, workflow_input: ExecutiveSummaryInput):
    """D√©marre un workflow Executive Summary"""
    try:
        # Cr√©er ou r√©cup√©rer le workflow
        if thread_id not in executive_workflows:
            api_key = os.getenv("OPENAI_API_KEY")
            workflow = ExecutiveSummaryWorkflow(
                api_key=api_key,
                dev_mode=False
            )
            executive_workflows[thread_id] = {
                "workflow": workflow,
                "state": None,
                "status": "created"
            }
        
        workflow_data = executive_workflows[thread_id]
        workflow = workflow_data["workflow"]
        
        print(f"\nüöÄ [API] D√©marrage workflow Executive Summary pour thread {thread_id}")
        
        # Ex√©cuter le workflow
        result = workflow.run(
            transcript_document_ids=workflow_input.transcript_document_ids,
            workshop_document_ids=workflow_input.workshop_document_ids,
            company_name=workflow_input.company_name,
            interviewer_note=workflow_input.interviewer_note,
            thread_id=thread_id,
            validated_needs=workflow_input.validated_needs,
            validated_use_cases=workflow_input.validated_use_cases
        )
        
        # Mettre √† jour l'√©tat
        workflow_data["state"] = result
        
        # D√©terminer le statut
        if result.get("workflow_paused"):
            validation_type = result.get("validation_type", "")
            if validation_type == "challenges":
                workflow_data["status"] = "waiting_validation_challenges"
            elif validation_type == "recommendations":
                workflow_data["status"] = "waiting_validation_recommendations"
            else:
                workflow_data["status"] = "paused"
        else:
            # Par d√©faut, le workflow est en cours d'ex√©cution
            # (le statut sera mis √† jour par get_executive_status en fonction de snapshot.next)
            workflow_data["status"] = "running"
        
        return {
            "run_id": str(uuid.uuid4()),
            "thread_id": thread_id,
            "status": workflow_data["status"]
        }
    
    except Exception as e:
        print(f"‚ùå [API] Erreur Executive Summary: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur workflow: {str(e)}")


@app.get("/executive-summary/threads/{thread_id}/status")
async def get_executive_status(thread_id: str):
    """R√©cup√®re le statut du workflow Executive Summary"""
    if thread_id not in executive_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = executive_workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # R√©cup√©rer l'√©tat actuel depuis le checkpointer
    config = {"configurable": {"thread_id": thread_id}}
    snapshot = workflow.graph.get_state(config)
    
    if snapshot and snapshot.values:
        state = snapshot.values
        
        # V√©rifier si on est √† un interrupt en regardant snapshot.next
        next_nodes = []
        if snapshot.next:
            if isinstance(snapshot.next, (list, tuple)):
                next_nodes = list(snapshot.next)
            else:
                next_nodes = [snapshot.next]
        
        # PRIORIT√â 1: Si pas de n≈ìuds suivants, le workflow est termin√©
        if not next_nodes or len(next_nodes) == 0:
            workflow_data["status"] = "completed"
            state["workflow_paused"] = False
            state["validation_type"] = ""
            workflow.graph.update_state(config, state)
            workflow_data["state"] = state
        # PRIORIT√â 2: Si le prochain n≈ìud est une validation ou un interrupt, mettre √† jour les flags
        elif "human_validation_enjeux" in next_nodes:
            state["workflow_paused"] = True
            state["validation_type"] = "challenges"
            workflow.graph.update_state(config, state)
            workflow_data["state"] = state
            workflow_data["status"] = "waiting_validation_challenges"
        elif "pre_recommendations_interrupt" in next_nodes:
            state["workflow_paused"] = True
            state["validation_type"] = "pre_recommendations"
            workflow.graph.update_state(config, state)
            workflow_data["state"] = state
            workflow_data["status"] = "waiting_pre_recommendations_context"
        elif "human_validation_recommendations" in next_nodes:
            state["workflow_paused"] = True
            state["validation_type"] = "recommendations"
            workflow.graph.update_state(config, state)
            workflow_data["state"] = state
            workflow_data["status"] = "waiting_validation_recommendations"
        # PRIORIT√â 3: Il y a des n≈ìuds suivants, le workflow est en cours
        else:
            workflow_data["status"] = "running"
            workflow_data["state"] = state
    
    return {"status": workflow_data["status"]}


@app.get("/executive-summary/threads/{thread_id}/state")
async def get_executive_state(thread_id: str):
    """R√©cup√®re l'√©tat du workflow Executive Summary"""
    if thread_id not in executive_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = executive_workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # R√©cup√©rer l'√©tat actuel depuis le checkpointer
    config = {"configurable": {"thread_id": thread_id}}
    snapshot = workflow.graph.get_state(config)
    
    # Prioriser l'√©tat du checkpointer, mais utiliser aussi l'√©tat stock√© dans workflow_data
    state_from_checkpointer = None
    if snapshot and snapshot.values:
        state_from_checkpointer = snapshot.values
    
    state_from_storage = workflow_data.get("state", {})
    
    # Fusionner les deux √©tats (priorit√© au checkpointer, mais compl√©ter avec storage)
    if state_from_checkpointer:
        state = state_from_checkpointer.copy()
        # Compl√©ter avec les donn√©es de storage si manquantes dans checkpointer
        for key in ["validated_challenges", "validated_recommendations", "maturity_score", "maturity_summary"]:
            if not state.get(key) and state_from_storage.get(key):
                state[key] = state_from_storage[key]
    else:
        state = state_from_storage
    
    # Debug: afficher ce qui est retourn√©
    print(f"üìä [API] √âtat retourn√© pour thread {thread_id}:")
    print(f"   - validated_challenges: {len(state.get('validated_challenges', []))}")
    print(f"   - validated_recommendations: {len(state.get('validated_recommendations', []))}")
    
    # Convertir en format JSON-serializable
    return {
        "identified_challenges": state.get("identified_challenges", []),
        "validated_challenges": state.get("validated_challenges", []),
        "extracted_needs": state.get("extracted_needs", []),
        "challenges_iteration_count": state.get("challenges_iteration_count", 0),
        "maturity_score": state.get("maturity_score", 3),
        "maturity_summary": state.get("maturity_summary", ""),
        "recommendations": state.get("recommendations", []),
        "validated_recommendations": state.get("validated_recommendations", []),
        "workflow_paused": state.get("workflow_paused", False),
        "validation_type": state.get("validation_type", "")
    }


@app.post("/executive-summary/threads/{thread_id}/continue")
async def continue_executive(thread_id: str, context_data: dict):
    """Continue le workflow apr√®s l'interrupt pre_recommendations"""
    import time
    api_start_time = time.time()
    print(f"‚è±Ô∏è [TIMING] continue_executive - D√âBUT ({time.strftime('%H:%M:%S.%f', time.localtime(api_start_time))[:-3]})")
    
    if thread_id not in executive_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    try:
        workflow_data = executive_workflows[thread_id]
        workflow = workflow_data["workflow"]
        config = {"configurable": {"thread_id": thread_id}}
        
        # R√©cup√©rer le feedback depuis le body de la requ√™te
        recommendations_feedback = context_data.get("recommendations_feedback", "")
        
        print(f"üìù [API] Feedback re√ßu: {recommendations_feedback[:100]}...")
        
        # Mettre √† jour l'√©tat avec le feedback
        get_state_start = time.time()
        snapshot = workflow.graph.get_state(config)
        get_state_duration = time.time() - get_state_start
        print(f"‚è±Ô∏è [TIMING] get_state: {get_state_duration:.3f}s")
        
        if snapshot and snapshot.values:
            state = snapshot.values
            # Accumuler le feedback (ne pas √©craser si d√©j√† pr√©sent)
            existing_feedback = state.get("recommendations_feedback", "")
            if recommendations_feedback:
                if existing_feedback:
                    state["recommendations_feedback"] = f"{existing_feedback}\n\n{recommendations_feedback}"
                else:
                    state["recommendations_feedback"] = recommendations_feedback
            state["workflow_paused"] = False
            state["validation_type"] = ""
            
            update_state_start = time.time()
            workflow.graph.update_state(config, state)
            update_state_duration = time.time() - update_state_start
            print(f"‚è±Ô∏è [TIMING] update_state: {update_state_duration:.3f}s")
        
        # Reprendre le workflow
        stream_start = time.time()
        final_state = None
        for chunk in workflow.graph.stream(None, config):
            print(f"üìä [EXECUTIVE] Chunk re√ßu apr√®s continue: {list(chunk.keys())}")
            for node_name, node_state in chunk.items():
                print(f"  ‚Ä¢ N≈ìud '{node_name}' ex√©cut√©")
                final_state = node_state
        
        stream_duration = time.time() - stream_start
        print(f"‚è±Ô∏è [TIMING] workflow.graph.stream: {stream_duration:.3f}s")
        
        # R√©cup√©rer l'√©tat final
        get_state_after_start = time.time()
        snapshot = workflow.graph.get_state(config)
        get_state_after_duration = time.time() - get_state_after_start
        print(f"‚è±Ô∏è [TIMING] get_state (apr√®s stream): {get_state_after_duration:.3f}s")
        
        if snapshot and snapshot.values:
            state = snapshot.values
            workflow_data["state"] = state
            
            # V√©rifier si on est √† un interrupt
            is_at_interrupt = False
            if snapshot.next:
                next_nodes = list(snapshot.next) if hasattr(snapshot.next, '__iter__') else [snapshot.next]
                if "human_validation_recommendations" in next_nodes:
                    is_at_interrupt = True
                    state["workflow_paused"] = True
                    state["validation_type"] = "recommendations"
                    workflow.graph.update_state(config, state)
                    print(f"üõë [API] Workflow arr√™t√© √† l'interrupt: {next_nodes}")
            
            # Mettre √† jour le statut
            if state.get("workflow_paused") or is_at_interrupt:
                validation_type = state.get("validation_type", "")
                if validation_type == "recommendations":
                    workflow_data["status"] = "waiting_validation_recommendations"
                else:
                    workflow_data["status"] = "paused"
            elif not snapshot.next:
                workflow_data["status"] = "completed"
            else:
                workflow_data["status"] = "running"
        elif final_state:
            workflow_data["state"] = final_state
            workflow_data["status"] = "running"
        
        total_duration = time.time() - api_start_time
        print(f"‚è±Ô∏è [TIMING] continue_executive (total): {total_duration:.3f}s")
        
        return {
            "status": "success",
            "workflow_status": workflow_data["status"],
            "message": "Workflow repris avec succ√®s"
        }
        
    except Exception as e:
        print(f"‚ùå [API] Erreur lors de la reprise: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la reprise: {str(e)}")

@app.post("/executive-summary/threads/{thread_id}/validate")
async def validate_executive(thread_id: str, feedback: ExecutiveValidationFeedback):
    """Valide les enjeux ou recommandations"""
    if thread_id not in executive_workflows:
        raise HTTPException(status_code=404, detail="Thread non trouv√©")
    
    workflow_data = executive_workflows[thread_id]
    workflow = workflow_data["workflow"]
    
    # Injecter le feedback dans l'√©tat
    import time
    api_start_time = time.time()
    print(f"‚è±Ô∏è [TIMING] validate_executive - D√âBUT ({time.strftime('%H:%M:%S.%f', time.localtime(api_start_time))[:-3]})")
    
    config = {"configurable": {"thread_id": thread_id}}
    get_state_start = time.time()
    current_state = workflow.graph.get_state(config)
    get_state_duration = time.time() - get_state_start
    print(f"‚è±Ô∏è [TIMING] get_state: {get_state_duration:.3f}s")
    
    # Mettre √† jour avec le feedback
    update_start = time.time()
    updated_state = current_state.values.copy()
    updated_state["validation_result"] = feedback.validation_result
    updated_state["validation_type"] = feedback.validation_type
    
    # Reprendre le workflow
    workflow.graph.update_state(config, updated_state)
    update_duration = time.time() - update_start
    print(f"‚è±Ô∏è [TIMING] update_state: {update_duration:.3f}s")
    
    # Continuer l'ex√©cution jusqu'au prochain interrupt
    stream_start = time.time()
    final_state = None
    for chunk in workflow.graph.stream(None, config):
        print(f"üìä [EXECUTIVE] Chunk re√ßu apr√®s validation: {list(chunk.keys())}")
        for node_name, node_state in chunk.items():
            print(f"  ‚Ä¢ N≈ìud '{node_name}' ex√©cut√©")
            final_state = node_state
    
    stream_duration = time.time() - stream_start
    print(f"‚è±Ô∏è [TIMING] workflow.graph.stream: {stream_duration:.3f}s")
    
    # R√©cup√©rer l'√©tat complet depuis le checkpointer apr√®s l'ex√©cution
    # IMPORTANT : Le workflow s'arr√™te √† human_validation_enjeux gr√¢ce √† interrupt_before
    get_state_after_start = time.time()
    snapshot = workflow.graph.get_state(config)
    get_state_after_duration = time.time() - get_state_after_start
    print(f"‚è±Ô∏è [TIMING] get_state (apr√®s stream): {get_state_after_duration:.3f}s")
    
    if snapshot and snapshot.values:
        state = snapshot.values
        workflow_data["state"] = state
        
        # V√©rifier si on est √† un interrupt (workflow_paused ou next contient human_validation)
        is_at_interrupt = False
        if snapshot.next:
            next_nodes = list(snapshot.next) if hasattr(snapshot.next, '__iter__') else [snapshot.next]
            if "human_validation_enjeux" in next_nodes or "pre_recommendations_interrupt" in next_nodes or "human_validation_recommendations" in next_nodes:
                is_at_interrupt = True
                state["workflow_paused"] = True
                if "human_validation_enjeux" in next_nodes:
                    state["validation_type"] = "challenges"
                elif "pre_recommendations_interrupt" in next_nodes:
                    state["validation_type"] = "pre_recommendations"
                elif "human_validation_recommendations" in next_nodes:
                    state["validation_type"] = "recommendations"
                # Mettre √† jour l'√©tat dans le checkpointer
                workflow.graph.update_state(config, state)
                print(f"üõë [API] Workflow arr√™t√© √† l'interrupt: {next_nodes}")
        
        # Mettre √† jour le statut
        # PRIORIT√â 1: Si pas de n≈ìuds suivants (snapshot.next est vide), le workflow est termin√©
        if not snapshot.next or (hasattr(snapshot.next, '__len__') and len(snapshot.next) == 0):
            workflow_data["status"] = "completed"
            state["workflow_paused"] = False
            state["validation_type"] = ""
            workflow.graph.update_state(config, state)
        # PRIORIT√â 2: V√©rifier si on est √† l'interrupt ou en pause
        elif state.get("workflow_paused") or is_at_interrupt:
            validation_type = state.get("validation_type", "")
            if validation_type == "challenges":
                workflow_data["status"] = "waiting_validation_challenges"
            elif validation_type == "recommendations":
                workflow_data["status"] = "waiting_validation_recommendations"
            else:
                workflow_data["status"] = "paused"
        else:
            workflow_data["status"] = "running"
    elif final_state:
        workflow_data["state"] = final_state
        workflow_data["status"] = "running"
    
    total_duration = time.time() - api_start_time
    print(f"‚è±Ô∏è [TIMING] validate_executive (total): {total_duration:.3f}s")
    
    return {"status": "success", "workflow_status": workflow_data["status"]}


if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=2025,
        log_level="info"
    )

